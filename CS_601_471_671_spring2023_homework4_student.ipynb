{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackychen08/ML_Homework/blob/master/CS_601_471_671_spring2023_homework4_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> CSCI 601.471/671 NLP: Self-supervised Models </h1> </center>\n",
        "\n",
        "<center> <h2> Homework 4: Counting-based and Neural Fixed-Window Language Models  </h2> </center>\n",
        "\n",
        "In this homework, we will build multiple language models. \n",
        "\n",
        "**After this assignment, you will be able to:**\n",
        "\n",
        "- implement n-gram language models \n",
        "- implement fixed-window (feedforward) language models \n",
        "- measure the quality of language modeling with perplexity \n",
        "- generate language from language model \n",
        "\n",
        "\n",
        "# Setup\n",
        "\n",
        "For this and other assignments, we will use Google Colab for both code and descriptive questions. Your task is to finish all the questions in the Colab notebook and then upload a PDF version of the notebook, and a viewable link on Gradescope.\n",
        "\n",
        "\n",
        "#### Google collaboratory\n",
        "\n",
        "Before getting started, get familiar with google colaboratory:\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "This neat python environment works in the cloud and does not require you to set up anything on your personal machine (it also has some built-in IDE features that make writing code easier). \n",
        "Moreover, it allows you to copy any existing collaboratory file, alter it and share\n",
        "with other people. In this homework, we will ask you to copy current colaboraty,\n",
        "complete all the tasks and share your collaboratory notebook with us so\n",
        "that we can grade it. \n",
        "\n",
        "\n",
        "### Submission\n",
        "\n",
        "Before you start working on this homework do the following steps:\n",
        "\n",
        "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
        "2. Follow all the steps in this collaboratory file and write / change / uncomment code as necessary.\n",
        "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
        "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected. Copy the link and paste it in the box below.\n",
        "5. After completing the notebook, press __File > Download .ipynb__ to download a local copy on your computer, and then upload the file to Gradescope.\n",
        "\n",
        "\n",
        "__Paste your notebook link in the box below.__ _(0 points)_"
      ],
      "metadata": {
        "id": "bUnwynBDi599"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste your Colab notebook link here "
      ],
      "metadata": {
        "id": "rB1K7UB_i-C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install the dependencies needed for this programming assignemnt. "
      ],
      "metadata": {
        "id": "Rq8KiIZSjFqa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7j-Zt7tZ0gY"
      },
      "outputs": [],
      "source": [
        "!python3 --version # to check the Python version. We have used Python 3.8.10 when preparing this assignemnt \n",
        "\n",
        "!pip install --upgrade pip # updates your pip tool to its latest version \n",
        "!pip install numpy==1.23.5 # contains many useful numercial tools \n",
        "!pip install tqdm==4.64.1 # a nice library for visualizing a progress bar \n",
        "!pip install torch==1.13.1\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm # progress bar \n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "print(torch.__version__) # this code was prepared with torch==1.13.1+cu116\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also make sure that you have GPU in your environment. "
      ],
      "metadata": {
        "id": "G61WCwGcAt_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.cuda.is_available(), \"You need to have GPU running in your enviroment.\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "dKsOnccPAtyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing our pre-training data\n",
        "Now let's download a dataset of raw sentences. \n",
        "In particular, we will use a subset of sentences extracted from Wikipedia. "
      ],
      "metadata": {
        "id": "GZ3Q1io4Aztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==2.9.0 # huggingface's library of datasets \n",
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(path=\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\") \n",
        "dev_dataset = load_dataset(path=\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"validation\") \n",
        "# test_dataset = load_dataset(path=\"wikitext\", name=\"wikitext-103-v1\", split=\"test\") "
      ],
      "metadata": {
        "id": "vw0orS2No-wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load any of the paragraphs from this dataset"
      ],
      "metadata": {
        "id": "Kx-95hGOsuZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['text'][10]"
      ],
      "metadata": {
        "id": "zBH54Yo8stVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and split it into sentences based on [an algorithm](https://aclanthology.org/W07-0733/) by our very own Philipp Koehn for sentence splitting. (see [here](https://github.com/mediacloud/sentence-splitter) for more documentation)"
      ],
      "metadata": {
        "id": "qMRLycKustwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-splitter==1.4 # for splitting paragraphs into sentences s\n",
        "from sentence_splitter import SentenceSplitter \n",
        "\n",
        "splitter = SentenceSplitter(language='en')\n",
        "for sentence in splitter.split(text=train_dataset['text'][10]): \n",
        "  print(f\" -> {sentence}\")"
      ],
      "metadata": {
        "id": "ve11-Hhpst3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also use a sub-word tokenizer. For now, we are using an existing sub-word tokenizer of an existing model from the Huggingface library, but in the future we will also build our own tokenizer. "
      ],
      "metadata": {
        "id": "nNdiwqqpecAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers \n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "zoQtupQ40lcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use this model to split each sentence into tokens using an existing tokenizer:\n",
        "\n"
      ],
      "metadata": {
        "id": "WxEZROz92UWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(\"To learn Battle Potentials , each character has a unique \\\" Masters Table \\\" , a grid @-@ based skill table that can be used to acquire and link different skills .\")\n",
        "print(f\" Tokens: {tokens}\")\n",
        "print(f\" Tokens to indices: {tokenizer.convert_tokens_to_ids(tokens)} \")"
      ],
      "metadata": {
        "id": "PcidMNas0uRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a **Sub-word tokenizer** which, as it should be clear from the name, splits each sentence into units smaller than words, based on their frequency. For example, the word \"Potentials\" is broken into four sub-words: `Po', '##ten', '##tial', '##s'`, where `##` is a special symbol indicating that the sub-word is in the middle of a word.\n",
        "\n",
        "\n",
        "Sub-word tokenization might initially seem like a bad idea since we are breaking up the word. But it brings it several benefits.\n",
        "\n",
        "\n",
        " - Handling Out-of-Vocabulary (OOV) words: Sub-word tokenization allows the representation of rare or unseen words as a combination of sub-words or tokens that have been seen in the training data, thus reducing the OOV problem.\n",
        " - Improved vocabulary size: It allows for a more compact vocabulary size, reducing the size of the language model, and increasing its efficiency.\n",
        " - Better language model performance: Sub-word tokenization results in a better representation of morphologically rich languages, where words are formed by combining roots and affixes, leading to improved language model performance.\n",
        " - Cross-lingual compatibility: Sub-word tokenization is language-agnostic, and models trained on sub-word tokens from one language can be applied to other languages, improving cross-lingual compatibility.\n",
        "\n",
        "How are sub-tokenizers built? We will delve into that next week! For now, we will just use them! "
      ],
      "metadata": {
        "id": "cRS5kGsD2qWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use our sentence splitter and tokenizer to preprocess our data. \n",
        "We will follow the same setup we saw in the class for \"fixed-window LMs\" where given a window of context words `local_window_size`, we would like to predict the word appearing after the window: \n",
        "\n",
        "<div>\n",
        "<img src=\"https://self-supervised.cs.jhu.edu/sp2023/files/fixed-window.png\" width=\"580\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "## **Question 1:** Complete the following code for preparing the pre-training code. "
      ],
      "metadata": {
        "id": "irEfHXdcst8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, local_window_size):\n",
        "  x_data = []\n",
        "  y_data = []\n",
        "  for paragraph in tqdm(data['text']):\n",
        "    \n",
        "    # if the paragraph is too short, skip it \n",
        "    if len(paragraph) < 5: \n",
        "      continue \n",
        "      \n",
        "    # iterate over sentences given by our sentence splitter \n",
        "    for sentence in splitter.split(paragraph): \n",
        "      \n",
        "      # tokenize the words in the our sentence \n",
        "      tokens = tokenizer.tokenize(sentence)\n",
        "      tokenIds = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "      # drop short sentences\n",
        "      if len(tokens) < local_window_size + 1: \n",
        "        break  \n",
        "      \n",
        "      for idx, _ in enumerate(tokenIds): \n",
        "        \n",
        "        if idx + local_window_size >= len(tokens): \n",
        "            # have already traversed all of the sentence \n",
        "            break\n",
        "\n",
        "        ######## START CODE HERE ########\n",
        "        # Select a subset of tokenIds from idx -> idx + local_window_size as input and put it to x \n",
        "        # Then select the word immediately after this window as output and put it to y \n",
        "        x = ...\n",
        "        y = ...\n",
        "        ######## END CODE HERE ########\n",
        "        \n",
        "        x_data.append(x)\n",
        "        y_data.append(y)\n",
        "  \n",
        "  # making numpy arrays\n",
        "  x_data = np.array(x_data)\n",
        "  y_data = np.array(y_data)\n",
        "  \n",
        "  return x_data, y_data \n"
      ],
      "metadata": {
        "id": "ps0Gn6keaTXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's callour data pre-processing function, and get the processed data. \n",
        "This should take less than 10 minutes. "
      ],
      "metadata": {
        "id": "LEGUpXQ6kVG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# local_window_size determines the size of the context \n",
        "local_window_size = 6 \n",
        "\n",
        "# creating training and dev dataset\n",
        "x_train, y_train = preprocess_data(\n",
        "      train_dataset[-500000:],  # use a subset of the paragraphs to save us some time \n",
        "      local_window_size \n",
        "    )\n",
        "x_dev, y_dev = preprocess_data(dev_dataset, local_window_size)\n",
        "\n",
        "  \n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_dev.shape)\n",
        "print(y_dev.shape)"
      ],
      "metadata": {
        "id": "hzLJTLulkVMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the dimension of our pre-training data, you should see that we have over 25M 6-gram inputs and outputs which we will use for training our models. This is great! \n",
        "\n",
        "\n",
        "Another issue that we discussed is batching instances. Since tensor operations are fast in GPUs, we want to batch instances so that we process multiple of them together.  \n",
        "\n",
        "<div>\n",
        "<img src=\"https://self-supervised.cs.jhu.edu/sp2023/files/batching.png\" width=\"580\"/>\n",
        "</div>\n",
        "\n",
        "Here we will use PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html )s in order for easy batching of instances. Note since our input-outputs are short (6 input and 1 output) we can batch MANY instances to fit in a GPU. Here we will go with 4096 but you probably can bump it higher, if you feel adventurous! 🚀"
      ],
      "metadata": {
        "id": "C0nlWvdpNQRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096\n",
        "\n",
        "train = np.concatenate((x_train, y_train), axis=1)\n",
        "dev = np.concatenate((x_dev, y_dev), axis=1)\n",
        "\n",
        "# Use PyTorch's internal functions for loading data and iterating over it \n",
        "# See more details here: https://pytorch.org/docs/stable/data.html \n",
        "from torch.utils.data import DataLoader \n",
        "\n",
        "train_loader = DataLoader(train, batch_size = batch_size)\n",
        "dev_loader = DataLoader(dev, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "I1raHH54C3rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be wondering why we chose a batch size that is the power of 2 (4096 = 2^12). Batch sizes that are powers of 2 are often used in deep learning because it is more computationally efficient for certain hardware configurations. The reason for this is that modern computer processors are optimized for working with data in memory that is a multiple of the size of the processor's cache lines. When the batch size is a power of 2, it ensures that data is loaded into the cache in an optimal manner, which can result in faster processing times. Additionally, using a power of 2 for the batch size can also help to minimize memory fragmentation, which can also impact processing performance.\n",
        "\n",
        "However, it's worth noting that the ideal batch size will vary depending on the specific hardware and software configuration, as well as the size of the model and the data. Ultimately, the best batch size will depend on a trade-off between computational efficiency and the ability to capture patterns in the data. In practice, researchers and practitioners will often experiment with a range of batch sizes to determine the one that works best for their particular use case.\n",
        "\n",
        "---\n",
        "\n",
        "Just to make sure our data looks the way we expected, you can sample one example from our data loaders: "
      ],
      "metadata": {
        "id": "esNCaAGENNu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = next(iter(train_loader))  # sample \n",
        "\n",
        "input = x[:,0:local_window_size]\n",
        "target = x[:,local_window_size]\n",
        "\n",
        "print(f\"Batched input-outputs: {x}\")\n",
        "print(f\"Batched inputs: {input}\")\n",
        "print(f\"Batched outputs: {target}\")"
      ],
      "metadata": {
        "id": "JOgqkRHfNOlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks reasonable in our eyes 👀 ! \n",
        "Hopefully, you like it too. \n",
        "\n",
        "\n",
        "# Fixed-Window LM\n",
        "\n",
        "Let's now turn to building our model. \n",
        "Here, we are following the footsteps of [this paper](https://arxiv.org/pdf/2104.03474.pdf), which is extending earlier studies such as [this](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).   \n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://camo.githubusercontent.com/c4d4a34f4a79956fb629d5951eb4a1fff3e5393a3468de48e572e03dbdfca02e/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e73696d656e6773756e2f6e706c6d2e706e67\" width=\"520\"/>\n",
        "</div>\n",
        "\n",
        "We will follow a modular design for our implementation to make it more interpretable. \n",
        "In particular, will implement 3 layers: \n",
        " - (1) The **input layer** which loops up word embeddings, concatenates them and transforms them into a hidden representation \n",
        " - (2) The **middle layer** transforms the representation with a non-linearity \n",
        " - (3) The **output layer** which transforms a hidden vector to a probability distribution over the words\n",
        "\n",
        "\n",
        "Let's get to work! \n",
        "\n",
        "\n",
        "## **Question 2:** Complete the following codes for implementing each layer of our neural network"
      ],
      "metadata": {
        "id": "VQli7qe3-T36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NPLM_first_block(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, local_window_size, hidden_size, dropout_p):\n",
        "        super(NPLM_first_block, self).__init__()\n",
        "        self.local_window_size = local_window_size # size of the context window \n",
        "        self.embedding_dim = embedding_dim # size of the word embeddings \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(local_window_size * embedding_dim, hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ######## START CODE HERE ########\n",
        "        # looking up the word embeddings from self.embeddings()\n",
        "        # And concatenating them \n",
        "        # Note this is done for a batch of instances. \n",
        "        # You may find .view() function useful: https://pytorch.org/docs/stable/generated/torch.Tensor.view.html \n",
        "        embeds = ...\n",
        "        \n",
        "        # Transform embeddings with tanh(W_1.x + b)\n",
        "        # Use self.linear as your linear transformation before tanh\n",
        "        transformed_embeds = ...\n",
        "                \n",
        "        # apply layer normalization \n",
        "        normalized_embeds = ...\n",
        "\n",
        "        # apply dropout \n",
        "        final_embeds = ...\n",
        "        ######## END END HERE ########\n",
        "\n",
        "        return final_embeds\n",
        "\n",
        "class NPLM_block(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, dropout_p):\n",
        "        super(NPLM_block, self).__init__()\n",
        "        self.hidden_size = hidden_size # size of the hidden representation \n",
        "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ######## START CODE HERE ########\n",
        "        # apply nonlinearity: tanh(W_1.x + b)\n",
        "        transformed_inputs = ...\n",
        "        \n",
        "        # add residual connection \n",
        "        transformed_inputs_with_residual = ...\n",
        "        \n",
        "        # apply layer normalization \n",
        "        inputs_normailzed = ...\n",
        "\n",
        "        # apply dropout \n",
        "        final_inputs = ...\n",
        "        ######## END END HERE ########\n",
        "\n",
        "        return final_inputs\n",
        "\n",
        "\n",
        "class NPLM_final_block(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(NPLM_final_block, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ######## START CODE HERE ########\n",
        "        # apply linear transformation: W.h\n",
        "        transformed_inputs = ...\n",
        "        \n",
        "        # appl log_softmax(W.h) to get log-probabilities (logits)\n",
        "        log_probs = ...\n",
        "        ######## END END HERE ########\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "fLZClppdacD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have different layers implemented, we can stack them to build our model. "
      ],
      "metadata": {
        "id": "0TQIoJrYrydP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NPLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, local_window_size, hidden_size, num_blocks, dropout_p):\n",
        "        super(NPLM, self).__init__()\n",
        "        \n",
        "        self.first_layer = NPLM_first_block(vocab_size, embedding_dim, local_window_size, hidden_size, dropout_p)\n",
        "        \n",
        "        self.intermediate_layers = nn.ModuleList()\n",
        "        \n",
        "        for i in range(num_blocks):\n",
        "            self.intermediate_layers.append(NPLM_block(hidden_size, dropout_p))\n",
        "        \n",
        "        self.final_layer = NPLM_final_block(vocab_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # input layer \n",
        "        out = self.first_layer(inputs)\n",
        "        \n",
        "        # multuple middle layers\n",
        "        for layer in self.intermediate_layers:\n",
        "            out = F.relu(layer(out)) # Optional ReLU here :) \n",
        "        \n",
        "        # output layer \n",
        "        log_probs = self.final_layer(out)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "d0mAuPB9vrWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3:** Reflect on `NPLM`'s implementation and explain your understanding (no more than 7 sentences). \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "moSP9cTysU6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here! "
      ],
      "metadata": {
        "id": "x2aMNvs1Gc1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://pbs.twimg.com/media/Dp-L3xpU8AAR8lp.jpg\" width=\"580\"/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "o6hPPb7DKfA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate our model to make sure there are no errors. "
      ],
      "metadata": {
        "id": "ZjJvnHElGiVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now instantiate our model \n",
        "embedding_dim = 256\n",
        "hidden_size = 1048\n",
        "num_blocks = 4\n",
        "dropout_p = 0.2\n",
        "\n",
        "\n",
        "model = NPLM(tokenizer.vocab_size, embedding_dim, local_window_size, hidden_size, num_blocks, dropout_p)\n",
        "model.to(device) # move it to GPU"
      ],
      "metadata": {
        "id": "VnauwDmDP1e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might wonder how big this model is. Let's count its parameters! "
      ],
      "metadata": {
        "id": "11S-koY0kHi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(f\"# of parameters: {params}\")"
      ],
      "metadata": {
        "id": "8rozizPhkLIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48M!! Not bad at all! Compared to the modern LMs, this is tiny. \n",
        "For example, GPT3 has 175B, about 4000x larger than our toy model. \n",
        "\n",
        "\n",
        "Now let's also give the model an input and see if it can process it successfully. "
      ],
      "metadata": {
        "id": "wC2XXznGtzXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = next(iter(train_loader))  # sample \n",
        "\n",
        "input = x[:,0:local_window_size]\n",
        "target = x[:,local_window_size]\n",
        "\n",
        "log_probs = model(input.to(device))\n",
        "\n",
        "print(f\"Batched inputs: {input}\")\n",
        "print(f\"Batched outputs: {target}\")\n",
        "print(f\"Model outpus: {log_probs}\")\n",
        "\n",
        "# the first dimension of the output should correspond to the batch size \n",
        "assert log_probs.shape[0] == target.shape[0] \n",
        "\n",
        "# the other dimension of the output should equal the vocab size since the model \n",
        "# produces a distribution over the vocabulary \n",
        "assert log_probs.shape[1] == tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "PeC1hUV_t3Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you get no errors, you might be good! \n",
        "\n",
        "Note that this model is not trained. So its output is somewhat random. \n",
        "\n",
        "\n",
        "# Training the model\n",
        "Now it's time to train and evaluate it! We will train our model with a trainer function that takes in the model, learning rate, and other parameters. \n",
        "\n",
        "Note we are using **Adam (Adaptive Moment Estimation)** [link text](https://arxiv.org/pdf/1412.6980), which is a popular optimization algorithm used in deep learning and machine learning. It is a stochastic gradient descent (SGD) optimization algorithm that is well suited for training deep neural networks. The algorithm has some internal estimates to dynamically adjust the learning rates for each parameter based on its past gradients, which can result in faster convergence and improved performance compared to traditional SGD."
      ],
      "metadata": {
        "id": "fNpT-MTbuT1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model, \n",
        "      train_loader, # training data \n",
        "      dev_loader, # dev data \n",
        "      num_epoch, # how any times we will see the whole data\n",
        "      lr, \n",
        "      decay, # how fast the learning rate will decay \n",
        "      criterion, # objective \n",
        "    ): \n",
        "  # hyperparameters\n",
        "  torch.manual_seed(42) # the answer to everything \n",
        "\n",
        "  # using ADAM optimizer\n",
        "  optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "  # scheduler, for adjusting learning rate \n",
        "  scheduler =  torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay)\n",
        "\n",
        "  best_ppl = np.inf\n",
        "  model_path = None\n",
        "\n",
        "  for epoch in range(num_epoch):\n",
        "      print(\"\\nTraining\")\n",
        "      for idx, data in enumerate(train_loader):\n",
        "          cur = data[:,0:local_window_size]\n",
        "          target = data[:,local_window_size]\n",
        "          cur, target = cur.to(device), target.to(device)\n",
        "          model.zero_grad()\n",
        "          \n",
        "          # get log probabilities over next words\n",
        "          log_probs = model(cur)\n",
        "          \n",
        "          # compute loss function\n",
        "          loss = criterion(log_probs, target)\n",
        "          \n",
        "          # extract perplexity \n",
        "          # remember the connection between perplexity and cross-entropy loss \n",
        "          ppl = torch.exp(loss)\n",
        "          \n",
        "          # backward pass and update gradient\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "          if idx % 100 == 0: \n",
        "              learning_rate = optimizer.param_groups[0]['lr']\n",
        "              print(\"Training Iteration {} of epoch {} complete. Loss: {}; Perplexity: {}; learning rate: {}\".format(idx, epoch, loss.item(), ppl, learning_rate))\n",
        "\n",
        "      print(\"\\nEvaluating\")\n",
        "      dev_loss, dev_ppl = evaluate(dev_loader, model, criterion, device)\n",
        "      print(\"Epoch {}: Dev Perplexity: {}; Dev Loss: {}\".format(epoch, dev_ppl, dev_loss))\n",
        "      if dev_ppl < best_ppl:\n",
        "          print(\"Best development perplexity improved from {} to {}, saving model...\".format(best_ppl, dev_ppl))\n",
        "          best_ppl = dev_ppl\n",
        "          # set best model path\n",
        "          model_path = 'best_model_{}.dat'.format(epoch)\n",
        "          # saving best model\n",
        "          torch.save(model.state_dict(), model_path)\n",
        "          # !cp f'/content/best_model_{epoch}.dat' '/content/gdrive/My Drive/fnnlmmodels/'"
      ],
      "metadata": {
        "id": "Ot2muZ-RapGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also implement a function to evaluate our model on the dev dataset. "
      ],
      "metadata": {
        "id": "c_LmK_i5v4pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model on dev data\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad(): # turn off gradient calculation because we want to evaluate the model \n",
        "        for idx, data_tensor in enumerate(dataloader):\n",
        "            context_tensor = data_tensor[:,0:local_window_size]\n",
        "            target_tensor = data_tensor[:,local_window_size]\n",
        "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "            log_probs = model(context_tensor)\n",
        "            loss += criterion(log_probs, target_tensor).item()\n",
        "            count += 1\n",
        "\n",
        "    return loss / count, np.exp(loss / count)"
      ],
      "metadata": {
        "id": "ZigFXsdiampc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 4:** Reflect on the implementation of `trainer` and how it works. Summarize your understanding of how it works here (no more than 10 sentences). \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nzpErYBAGsq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your answer here! "
      ],
      "metadata": {
        "id": "5tqG8D7ZG7mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train a model. \n",
        "\n",
        "Like the previous homework, we will use **cross-entropy loss** between the predictions of our language model and actual words that appeared in our training data. \n",
        "\n",
        "<div>\n",
        "<img src=\"https://self-supervised.cs.jhu.edu/sp2023/files/cross-entropy.png\" width=\"580\"/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "5AbfWrdd0e_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read more: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "dev_loss, dev_ppl = evaluate(dev_loader, model, criterion, device)\n",
        "print(\"Development perplexity {}...\".format(dev_ppl))\n",
        "\n",
        "# this may take up some time \n",
        "trainer(model, train_loader, dev_loader, num_epoch = 5, lr=2e-6, decay=1.0, criterion=criterion)"
      ],
      "metadata": {
        "id": "1-tZrL_kYNhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see consistent perplexity decrease, then you should be good! \n",
        "If not, it's possible that you made a mistake somewhere."
      ],
      "metadata": {
        "id": "qoaplB9m2BWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Pre-trained Models \n",
        "\n",
        "Training the above LM might take up to 10 hours to train. To save you time, we have trained a language model for you to play with. All you have to do is to download its weight parameters. \n",
        "\n",
        "\n",
        "Open the following file and copy it to your local directory under `/content/`.\n",
        "\n",
        "```\n",
        "https://livejohnshopkins-my.sharepoint.com/:u:/r/personal/dkhasha1_jh_edu/Documents/spring-2023-cs-601-471/others/pretrained_fixed_window_lm.dat?csf=1&web=1&e=nzgcaV\n",
        "```"
      ],
      "metadata": {
        "id": "UCwcBwDlGLGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And make sure that you can see it among your files. \n",
        "\n",
        "![Screen Shot 2023-02-12 at 7.55.11 AM.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "RhLEZBZ_sQkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the model weight downloaded, we can instantiate a model with these parameters. This will work in two steps. \n",
        " - First we will need to create a new model (with potentially random weights). \n",
        " - Then, we will copy the parameter values to the model using `load_state_dict()` function. You can read more about this function [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html).  "
      ],
      "metadata": {
        "id": "bTn0Hi4gISRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now instantiate our model \n",
        "embedding_dim = 256\n",
        "hidden_size = 1048\n",
        "num_blocks = 4\n",
        "dropout_p = 0.2\n",
        "\n",
        "\n",
        "pretrained_model = NPLM(tokenizer.vocab_size, embedding_dim, local_window_size, hidden_size, num_blocks, dropout_p)\n",
        "pretrained_model.to(device)\n",
        "\n",
        "checkpoint = torch.load('/content/pretrained_fixed_window_lm.dat', map_location=torch.device('cuda:0'))\n",
        "\n",
        "pretrained_model.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "id": "lD5wDDGHIRjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that parameters should match the architecture. For example, if `num_blocks ` was set to `3` instead of `4`, you'd see an error message from Pytorch. \n",
        "\n",
        "\n",
        "How good is our model? Let's evaluate it on the dev set! "
      ],
      "metadata": {
        "id": "D-HDXc_mJXam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_loss, dev_ppl = evaluate(dev_loader, pretrained_model, criterion, device)\n",
        "print(\"Development perplexity {}...\".format(dev_ppl))"
      ],
      "metadata": {
        "id": "CC-iSdEgJbXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is pretty good but certainly can be further improved! Can you beat it? "
      ],
      "metadata": {
        "id": "Hyt4d3MSJ4aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extra credit:** If you manage to improve the perplexity of the perplexity  by **at least** 10 points (better training or architecture hyperparams, etc.), you'll get 5 extra credit. Each additional 10-point improvement in perplexity will be awarded an additional 5 units of extra credit. So in an ideal world, if you reudce the PPL to 1, you'll get 125 extra credit points!!! 🤡"
      ],
      "metadata": {
        "id": "lDi7JFqMGHes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation from Language Models \n",
        "\n",
        "Now that we have trained our language model, how can we sample from it? \n",
        "\n",
        "Here are two strategies we will try to generate each word. \n",
        "(1) We can select the most probable words (argmax). \n",
        "<div>\n",
        "<img src=\"https://self-supervised.cs.jhu.edu/sp2023/files/argmax_sampling.webp\" width=\"580\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "(2) We can sample from the distribution proportional to word probablities. Words with higher probabilities will be more likely to be sampled. \n",
        "However, we also have an option of filter the low-probability tokens, and instead retaining tokens that constitute `top_p` probability.  \n",
        "<div>\n",
        "<img src=\"https://self-supervised.cs.jhu.edu/sp2023/files/p_sampling.webp\" width=\"580\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "Let's give implement these! \n",
        " "
      ],
      "metadata": {
        "id": "9m3LE7UAE6Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import top_k_top_p_filtering\n",
        "\n",
        "\n",
        "def generate_text(prompt, model, top_p, maximum_generation = 100): \n",
        "\n",
        "  # tokenize the text and turn it into indices \n",
        "  tokenized_prompt = tokenizer.tokenize(prompt)\n",
        "  tokenized_prompt_ids = tokenizer.convert_tokens_to_ids(tokenized_prompt) \n",
        "\n",
        "  pred = 0\n",
        "  count = 0\n",
        "  while count < maximum_generation:\n",
        "      # select the tokens in the window \n",
        "      new_input = torch.tensor(tokenized_prompt_ids[-local_window_size:]).to(device)\n",
        "      \n",
        "      # compute model output \n",
        "      output = model(new_input)\n",
        "      \n",
        "      if top_p:   \n",
        "        # filter the top_p generations \n",
        "        output = top_k_top_p_filtering(output, top_p=top_p)\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        pred = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "      else: \n",
        "        # greedy decoding \n",
        "        pred = torch.argmax(output).item()\n",
        "    \n",
        "      tokenized_prompt_ids.append(pred)\n",
        "      count += 1\n",
        "  \n",
        "  # turn token-ids to their strings\n",
        "  subwords = tokenizer.convert_ids_to_tokens(tokenized_prompt_ids)\n",
        "\n",
        "  # turn sub-words into a sentence\n",
        "  generation = \"\"\n",
        "  for token in subwords:\n",
        "      if token.startswith(\"##\"):\n",
        "        generation += token.replace(\"##\", \"\")\n",
        "      else: \n",
        "        generation += \" \" + token\n",
        "  print(f\"Generated text: {generation}\")\n"
      ],
      "metadata": {
        "id": "clQXqk3p3dr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The best perks of living on the east\"\n",
        "\n",
        "\n",
        "# greedy decoding \n",
        "print(\" -------- try #1 --------\")\n",
        "generate_text(prompt=prompt, model=pretrained_model, top_p=None)\n",
        "\n",
        "# sampling with p=0.0; note this is equivalent to the greedy search \n",
        "print(\" -------- try #2 --------\")\n",
        "generate_text(prompt=prompt, model=pretrained_model, top_p=0.0)\n",
        "\n",
        "# sampling with p=0.3\n",
        "print(\" -------- try #3 --------\")\n",
        "generate_text(prompt=prompt, model=pretrained_model, top_p=0.3)\n",
        "\n",
        "# sampling with p=1.0 \n",
        "print(\" -------- try #5 --------\")\n",
        "generate_text(prompt=prompt, model=pretrained_model, top_p=1.0)"
      ],
      "metadata": {
        "id": "M6UozoxCwh3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5:** Compare the output of the above generations? Which one is your favorite. Explain why this choice of sampling leads to better text generation. "
      ],
      "metadata": {
        "id": "7OFaDm9BCvko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously, these generations are nowhere close to perfect and there are various reasons for that.  First, our model is quite small compared to many modern NLP models. The data we use is small as it is just a fractin of Wikipedia, while modern NLP models are trained on much larger data. So, this is really not bad at all! "
      ],
      "metadata": {
        "id": "MH3FTAv5jsgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obama LM\n",
        "\n",
        "Just for fun, we will tune our language model on Obama's speeches to see if our model can immitate Obama's style of speaking! \n",
        "\n",
        "<div>\n",
        "<img src=\"https://media-cldnry.s-nbcnews.com/image/upload/newscms/2017_02/1861016/170110-obama-farewell-rhk.jpg\" width=\"520\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn6jDObgh9C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download a collection of Obama's speeches: "
      ],
      "metadata": {
        "id": "083mQXouGFGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://self-supervised.cs.jhu.edu/sp2023/files/\"\n",
        "dataset = load_dataset(\"text\", data_files={'train': base_url + \"obama-speeches-train.txt\", 'dev':  base_url + \"obama-speeches-test.txt\"})\n",
        "obama_train_dataset = dataset['train']\n",
        "obama_dev_dataset = dataset['dev']"
      ],
      "metadata": {
        "id": "_L7Hs2GRiBTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And pre-process them the way we pre-processed the Wikipedia data. "
      ],
      "metadata": {
        "id": "BmVN55_miCnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training and dev dataset\n",
        "local_window_size = 6\n",
        "x_train, y_train = preprocess_data(obama_train_dataset, local_window_size)\n",
        "x_dev, y_dev = preprocess_data(obama_dev_dataset, local_window_size)\n",
        "\n",
        "  \n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_dev.shape)\n",
        "print(y_dev.shape)"
      ],
      "metadata": {
        "id": "Jxybk7x7kFOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And turn it into batches: "
      ],
      "metadata": {
        "id": "HFEAjRZ8HQ6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096\n",
        "\n",
        "obama_train = np.concatenate((x_train, y_train), axis=1)\n",
        "obama_dev = np.concatenate((x_dev, y_dev), axis=1)\n",
        "\n",
        "# Use PyTorch's internal functions for loading data and iterating over it \n",
        "# See more details here: https://pytorch.org/docs/stable/data.html \n",
        "from torch.utils.data import DataLoader \n",
        "\n",
        "obama_train_loader = DataLoader(obama_train, batch_size = batch_size)\n",
        "obama_dev_loader = DataLoader(obama_dev, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "Cw224k1EkPni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we can train a new model from scratch on this data, the Obama data is small and hence, training a new model is not going to work. \n",
        "Instead we will use our pretrained model on Wikipedia and do additional training on Obama speeches: "
      ],
      "metadata": {
        "id": "na_vg9UVHUZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer(pretrained_model, obama_train_loader, obama_dev_loader, num_epoch = 10, lr=1e-4, decay=1.0, criterion=criterion)"
      ],
      "metadata": {
        "id": "ejSI2djeiC92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to generate from this model. "
      ],
      "metadata": {
        "id": "YhpeImp8iDFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Hi, everybody. The obamacare will \", model, top_p=0.1)"
      ],
      "metadata": {
        "id": "lcM79w9ZiDM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huh ... interesting! Probably not the best Obama LM we will build, but not a bad start. We will be much better ones in the coming lectures. "
      ],
      "metadata": {
        "id": "VnCGBVYniDUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an N-Grams LMs\n",
        "\n",
        "We can also build an n-gram language model and compare it to our neural model. \n",
        "As a reminder, an n-gram language model is a statistical language model that predicts the next word in a sequence based on the preceding n-1 words. In other words, it models the probability distribution of sequences of words, where each sequence is represented as a set of n consecutive words, called an n-gram. The model is trained on a large corpus of text to learn the probabilities of observing different n-grams in the language.\n",
        "\n",
        "For example, in a bigram language model (n=2), the model would estimate the probability of observing each bigram (two-word sequence) in the language based on the training data. Given a sequence of words, the model would predict the next word in the sequence based on the preceding word. In a trigram language model (n=3), the model would predict the next word based on the preceding two words, and so on.\n",
        "\n",
        "\n",
        "It's actually easy to implement n-grams on a small scale. \n",
        "Here is our implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "l_Xv7UmXFshu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict # we will using \"Counter\" data structure for counting word co-occurences \n",
        "\n",
        "def create_ngrams(data, n): \n",
        "  ngrams = Counter()\n",
        "  ngram_context = Counter()\n",
        "  word_candidate = defaultdict(list)\n",
        "\n",
        "\n",
        "  for paragraph in tqdm(data['text']):\n",
        "      \n",
        "      # if the paragraph is too short, skip it \n",
        "      if len(paragraph) < 3: \n",
        "        continue \n",
        "        \n",
        "      # iterate over sentences given by our sentence splitter \n",
        "      for sentence in splitter.split(paragraph): \n",
        "        \n",
        "        # tokenize the words in the our sentence \n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        # tokenIds = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # drop short sentences\n",
        "        if len(tokens) < local_window_size + 1: \n",
        "          break  \n",
        "        \n",
        "        for idx, _ in enumerate(tokens): \n",
        "          \n",
        "          if idx + n >= len(tokens): \n",
        "              # have already traversed all of the sentence \n",
        "              break\n",
        "\n",
        "          ngrams[tuple(tokens[idx: idx+n])] += 1\n",
        "          ngram_context[tuple(tokens[idx: idx+n-1])] += 1\n",
        "          word_candidate[tuple(tokens[idx: idx+n-1])].append(tokens[idx+n-1])\n",
        "\n",
        "  # calculate tuple probability\n",
        "  word_pred = {}\n",
        "  for key, value in word_candidate.items():\n",
        "      scores = []\n",
        "      for i in value:\n",
        "          tmp_key = list(key)\n",
        "          tmp_key.append(i)\n",
        "          scores.append(ngrams[tuple(tmp_key)]/ngram_context[key])\n",
        "      word_pred[key] = value[np.argmax(scores)]\n",
        "  return word_pred"
      ],
      "metadata": {
        "id": "u7aSwOWs88fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use a subset to compute the 3-grams \n",
        "# even though this is a subset of the data, and only done for 3-grams, \n",
        "# it will take around 1-2 minutes. \n",
        "# note that this process is done on CPUs (it's all counting)\n",
        "word_pred = create_ngrams(train_dataset[:100000], n=3) "
      ],
      "metadata": {
        "id": "ysrcKZYyw_sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate\n",
        "n = 3 \n",
        "initial = tuple(\"The album by\".lower().split())\n",
        "res = list(initial)\n",
        "maximum = 100\n",
        "stop = \".\"\n",
        "pred = \"\"\n",
        "count = 0\n",
        "while count < maximum and pred != \".\":\n",
        "    pred = word_pred[tuple(res[-n+1:])]\n",
        "    res.append(pred)\n",
        "    count += 1\n",
        "print(\" \".join(res))"
      ],
      "metadata": {
        "id": "cnaIXcUBiDcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extra credit:** (5 points) write a modified implementation of n-gram LM to do p-sampling generation (e.g., p=0.3). "
      ],
      "metadata": {
        "id": "_BOfBncE7epJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Classification with Rich Features of LMs "
      ],
      "metadata": {
        "id": "24CMe1wpiDiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember we implemented a sentiment classifier with Word2Vec embeddings? \n",
        "We can repeat the same idea, but with richer features we have can obtain from our language model and get a better a classifir. We will not do that now since we have already done quite a bit already by now. However, we will revisit the idea in our future language models. "
      ],
      "metadata": {
        "id": "XKU8c2K4jA8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Congratulations!\n",
        "You've come to the end of this assignment! \n"
      ],
      "metadata": {
        "id": "QfuHoAPxjBEI"
      }
    }
  ]
}